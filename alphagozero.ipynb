{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gym\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, input_dims, n_filters):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(input_dims, n_filters, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(n_filters),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(n_filters, n_filters, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(n_filters),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.shortcut = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(input_dims, n_filters, kernel_size=1, stride=1),\n",
    "            torch.nn.BatchNorm2d(n_filters),\n",
    "        )\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.layers(x) + self.shortcut(x))\n",
    "\n",
    "\n",
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, board_size, input_dims):\n",
    "        super().__init__()\n",
    "        n_filters = 32\n",
    "        \n",
    "        self.main_path = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(input_dims[0], n_filters, kernel_size=3, stride=1),\n",
    "            torch.nn.BatchNorm2d(n_filters),\n",
    "            torch.nn.ReLU(),\n",
    "            ResidualBlock(n_filters, n_filters),\n",
    "            ResidualBlock(n_filters, n_filters),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.policy = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(n_filters, 2, kernel_size=1, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear((board_size-2)**2 * 2, board_size ** 2 + 1),\n",
    "            torch.nn.Softmax(dim=1),\n",
    "        )\n",
    "        \n",
    "        self.value = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(n_filters, 1, kernel_size=1, stride=1),\n",
    "            torch.nn.BatchNorm2d(1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear((board_size-2)**2, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 1),\n",
    "            torch.nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = self.main_path(state)\n",
    "        policy = self.policy(x)\n",
    "        value = self.value(x)\n",
    "        return policy, value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [black, white, turn, invalid, pass, game_over]\n",
    "class MCTSNode:\n",
    "    def __init__(self, state, parent=None, prior_action=-1):\n",
    "        size = state.shape[1] * state.shape[2] + 1 # board size + pass\n",
    "        self.valid_actions = (1 - state[3]).flatten() # invalid actions\n",
    "        self.valid_actions = torch.cat((self.valid_actions, torch.tensor([1.0]))) # with pass\n",
    "        self.valid_actions = self.valid_actions.bool()\n",
    "\n",
    "        self.visit_counts = torch.zeros(size, dtype=torch.int32)\n",
    "        self.total_action_values = torch.zeros(size, dtype=torch.float32)\n",
    "        self.mean_action_values = torch.zeros(size, dtype=torch.float32)\n",
    "        self.prior_probabilities = torch.zeros(size, dtype=torch.float32)\n",
    "        \n",
    "        self.selected_count = 0\n",
    "        self.children = [None] * size\n",
    "        \n",
    "        self.parent = parent\n",
    "        self.prior_action = prior_action\n",
    "        \n",
    "    def select(self, puct_multiplier=1.0):        \n",
    "        const_part = puct_multiplier * np.sqrt(self.selected_count)\n",
    "        us = const_part * self.prior_probabilities / (1 + self.visit_counts)\n",
    "        \n",
    "        sums = self.mean_action_values + us\n",
    "        invalid_value = torch.min(sums) - 1\n",
    "        masked_sums = torch.where(self.valid_actions, sums, invalid_value)\n",
    "        max_a = torch.argmax(masked_sums)\n",
    "        \n",
    "        return max_a, self.children[max_a]\n",
    "    \n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, state, evaluator, board_size=19):\n",
    "        self.root = MCTSNode(state)\n",
    "        self.PUCT_CONST = 1.0\n",
    "        self.tau = 1.0\n",
    "        \n",
    "        # dihedral transforms on the board\n",
    "        self.dihedrals_transforms = [\n",
    "            lambda x: x,\n",
    "            lambda x: torch.rot90(x, k=1, dims=(1, 2)),\n",
    "            lambda x: torch.rot90(x, k=2, dims=(1, 2)),\n",
    "            lambda x: torch.rot90(x, k=3, dims=(1, 2)),\n",
    "            lambda x: torch.flip(x, dims=(1,)),\n",
    "            lambda x: torch.flip(x, dims=(2,)),\n",
    "            lambda x: torch.flip(torch.rot90(x, k=1, dims=(1, 2)), dims=(1,)),\n",
    "            lambda x: torch.flip(torch.rot90(x, k=1, dims=(1, 2)), dims=(2,)),\n",
    "        ]\n",
    "        \n",
    "        self.evaluator = evaluator\n",
    "        self.board_size = board_size\n",
    "        \n",
    "    \n",
    "    def train(self, env, state):        \n",
    "        state, parent, child, action = self.search(env, self.root)\n",
    "        child, value = self.expand(state, parent, action)\n",
    "        self.backup(child, value)\n",
    "                \n",
    "    def search(self, env, root):\n",
    "        node = root\n",
    "        cnode = root\n",
    "        state = None\n",
    "        done = False\n",
    "        while cnode and not done:\n",
    "            node = cnode\n",
    "            action, cnode = node.select(self.PUCT_CONST)\n",
    "            state, _, done, _ = env.step(action.item())\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "        \n",
    "        return state, node, cnode, action\n",
    "    \n",
    "    def expand(self, state, parent, action):\n",
    "        dihedrals = [t(state) for t in self.dihedrals_transforms]\n",
    "        dihedrals = torch.stack(dihedrals)\n",
    "        policy, value = self.evaluator(dihedrals)\n",
    "        policy = policy.mean(dim=0)\n",
    "        value = value.mean()\n",
    "        \n",
    "        child = MCTSNode(state, parent=parent, prior_action=action)\n",
    "        parent.children[action] = child\n",
    "        child.prior_probabilities = policy\n",
    "        \n",
    "        return child, value\n",
    "        \n",
    "    def backup(self, child, value, child_was_none=False):\n",
    "        parent = child\n",
    "        if not child_was_none:\n",
    "            parent = child.parent\n",
    "        while parent:\n",
    "            parent.selected_count += 1\n",
    "            parent.visit_counts[child.prior_action] += 1\n",
    "            parent.total_action_values[child.prior_action] += value\n",
    "            parent.mean_action_values[child.prior_action] = parent.total_action_values[child.prior_action] / parent.visit_counts[child.prior_action]\n",
    "            \n",
    "            child, parent = parent, parent.parent\n",
    "            \n",
    "            \n",
    "    def play(self, state, deterministic=False):\n",
    "        node = self.root\n",
    "        pis = (node.visit_counts ** (1 / self.tau)) / (node.selected_count ** (1 / self.tau))\n",
    "        if deterministic:\n",
    "            selected_action = torch.argmax(pis)\n",
    "        else:\n",
    "            selected_action = torch.multinomial(pis, 1).item()\n",
    "        \n",
    "          \n",
    "        child = node.children[selected_action]\n",
    "        child.parent = None\n",
    "        self.root = child\n",
    "        return selected_action\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', 'state policy outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaGoZero:\n",
    "    def __init__(self, env, board_size=19):\n",
    "        self.env = env\n",
    "        self.board_size = board_size\n",
    "        \n",
    "        self.max_moves = board_size ** 2 + 1\n",
    "        \n",
    "        self.sub_iterations = 25\n",
    "        self.batch_size = 16\n",
    "        self.min_buffer_size = 16\n",
    "                \n",
    "        input_dims = env.observation_space.shape\n",
    "\n",
    "        base_state = env.reset()\n",
    "        base_state = torch.tensor(base_state, dtype=torch.float32)\n",
    "        self.dnn = DNN(board_size, input_dims)\n",
    "        self.mcts = MCTS(base_state, self.dnn, board_size)\n",
    "        self.trojectory_buffer = deque(maxlen=10000)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.dnn.parameters(), lr=0.0001)\n",
    "        \n",
    "    def train(self, iterations, render=False, eval=False):\n",
    "        for i in range(iterations):\n",
    "\n",
    "            state = self.env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            done = False\n",
    "        \n",
    "            side = -1\n",
    "            step_n = 0\n",
    "            print(\"Iteration: \", i)\n",
    "            \n",
    "            action_so_far = []\n",
    "            \n",
    "            while not done and step_n <= self.max_moves:\n",
    "                if render:\n",
    "                    print(\"Iteration: \", i, \"Step: \", step_n, \" Side: \", \"Black\" if side == -1 else \"White\", \"action: \")\n",
    "                env2 = gym.make('gym_go:go-v0', size=5, komi=0, reward_method='heuristic')\n",
    "                for _ in range(self.sub_iterations):\n",
    "                    # env2 = copy.deepcopy(self.env)\n",
    "                    env2.reset()\n",
    "                    \n",
    "                    for a in action_so_far:\n",
    "                        state, _, _, _ = env2.step(a)\n",
    "                        state = torch.tensor(state, dtype=torch.float32)\n",
    "                    try:\n",
    "                        self.mcts.train(env2, state)\n",
    "                    except Exception as e:\n",
    "                        env2.render(\"terminal\")\n",
    "                        raise e\n",
    "                env2.close()\n",
    "                \n",
    "                action = self.mcts.play(state, deterministic=eval)\n",
    "                action_so_far.append(action)\n",
    "                nstate, reward, done, _ = self.env.step(action)\n",
    "                if render:\n",
    "                    self.env.render(\"terminal\")\n",
    "                \n",
    "                self.trojectory_buffer.append(Experience(state, action, reward * side))\n",
    "                state = torch.tensor(nstate, dtype=torch.float32)\n",
    "                \n",
    "                side *= -1\n",
    "                step_n += 1\n",
    "            \n",
    "            if not eval:\n",
    "                self.train_dnn()\n",
    "            \n",
    "    def train_dnn(self):\n",
    "        if len(self.trojectory_buffer) < self.min_buffer_size:\n",
    "            return\n",
    "        \n",
    "        batches = random.sample(self.trojectory_buffer, self.batch_size)\n",
    "        states, policies, outcomes = zip(*batches)\n",
    "        states = torch.stack(states).detach()\n",
    "        policies = torch.tensor(policies, dtype=torch.long).detach()\n",
    "        outcomes = torch.tensor(outcomes, dtype=torch.float32).detach()\n",
    "        \n",
    "        # Forward pass\n",
    "        predicted_policies, predicted_values = self.dnn(states)\n",
    "        \n",
    "        # Define loss function (e.g., MSE for value and cross-entropy for policy)\n",
    "        value_loss = torch.nn.functional.mse_loss(predicted_values.squeeze(-1), outcomes)\n",
    "        policy_loss = torch.nn.functional.cross_entropy(predicted_policies, policies)\n",
    "        total_loss = value_loss + policy_loss\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Iteration:  1\n",
      "Iteration:  2\n",
      "Iteration:  3\n",
      "Iteration:  4\n",
      "Iteration:  5\n",
      "Iteration:  6\n",
      "Iteration:  7\n",
      "Iteration:  8\n",
      "Iteration:  9\n"
     ]
    }
   ],
   "source": [
    "SIZE = 5\n",
    "\n",
    "env = gym.make('gym_go:go-v0', size=SIZE, komi=0, reward_method='heuristic')\n",
    "alphago_zero = AlphaGoZero(env, board_size=SIZE)\n",
    "alphago_zero.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Iteration:  0 Step:  0  Side:  Black action: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Assignment\\Year_4\\70028\\rl\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:20: UserWarning: \u001b[33mWARN: It seems a Box observation space is an image but the `dtype` is not `np.uint8`, actual type: float32. If the Box observation space is not an image, we recommend flattening the observation to have only a 1D vector.\u001b[0m\n",
      "  logger.warn(\n",
      "d:\\Assignment\\Year_4\\70028\\rl\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:25: UserWarning: \u001b[33mWARN: It seems a Box observation space is an image but the upper and lower bounds are not in [0, 255]. Generally, CNN policies assume observations are within that range, so you may encounter an issue if the observation values are not.\u001b[0m\n",
      "  logger.warn(\n",
      "d:\\Assignment\\Year_4\\70028\\rl\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "d:\\Assignment\\Year_4\\70028\\rl\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "d:\\Assignment\\Year_4\\70028\\rl\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n",
      "d:\\Assignment\\Year_4\\70028\\rl\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "d:\\Assignment\\Year_4\\70028\\rl\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n",
      "d:\\Assignment\\Year_4\\70028\\rl\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:226: UserWarning: \u001b[33mWARN: Expects `done` signal to be a boolean, actual type: <class 'int'>\u001b[0m\n",
      "  logger.warn(\n",
      "d:\\Assignment\\Year_4\\70028\\rl\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "d:\\Assignment\\Year_4\\70028\\rl\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t0 1 2 3 4 \n",
      "0\t╔═╤═╤═╤═╗\n",
      "1\t╟─┼─┼─┼─╢\n",
      "2\t╟─┼─┼─┼─╢\n",
      "3\t╟─┼─┼─┼─╢\n",
      "4\t○═╧═╧═╧═╝\n",
      "\tTurn: WHITE, Game State (ONGOING|PASSED|END): ONGOING\n",
      "\tBlack Area: 25, White Area: 0\n",
      "\n",
      "Iteration:  0 Step:  1  Side:  White action: \n",
      "\t0 1 2 3 4 \n",
      "0\t╔═╤═╤═╤═╗\n",
      "1\t╟─┼─┼─┼─╢\n",
      "2\t╟─┼─┼─┼─╢\n",
      "3\t╟─┼─┼─┼─╢\n",
      "4\t○═╧═╧═●═╝\n",
      "\tTurn: BLACK, Game State (ONGOING|PASSED|END): ONGOING\n",
      "\tBlack Area: 1, White Area: 1\n",
      "\n",
      "Iteration:  0 Step:  2  Side:  Black action: \n",
      "\t0 1 2 3 4 \n",
      "0\t○═╤═╤═╤═╗\n",
      "1\t╟─┼─┼─┼─╢\n",
      "2\t╟─┼─┼─┼─╢\n",
      "3\t╟─┼─┼─┼─╢\n",
      "4\t○═╧═╧═●═╝\n",
      "\tTurn: WHITE, Game State (ONGOING|PASSED|END): ONGOING\n",
      "\tBlack Area: 2, White Area: 1\n",
      "\n",
      "Iteration:  0 Step:  3  Side:  White action: \n",
      "\t0 1 2 3 4 \n",
      "0\t○═╤═╤═╤═●\n",
      "1\t╟─┼─┼─┼─╢\n",
      "2\t╟─┼─┼─┼─╢\n",
      "3\t╟─┼─┼─┼─╢\n",
      "4\t○═╧═╧═●═╝\n",
      "\tTurn: BLACK, Game State (ONGOING|PASSED|END): ONGOING\n",
      "\tBlack Area: 2, White Area: 2\n",
      "\n",
      "Iteration:  0 Step:  4  Side:  Black action: \n",
      "\t0 1 2 3 4 \n",
      "0\t○═╤═╤═╤═●\n",
      "1\t○─┼─┼─┼─╢\n",
      "2\t╟─┼─┼─┼─╢\n",
      "3\t╟─┼─┼─┼─╢\n",
      "4\t○═╧═╧═●═╝\n",
      "\tTurn: WHITE, Game State (ONGOING|PASSED|END): ONGOING\n",
      "\tBlack Area: 3, White Area: 2\n",
      "\n",
      "Iteration:  0 Step:  5  Side:  White action: \n",
      "\t0 1 2 3 4 \n",
      "0\t○═╤═╤═●═●\n",
      "1\t○─┼─┼─┼─╢\n",
      "2\t╟─┼─┼─┼─╢\n",
      "3\t╟─┼─┼─┼─╢\n",
      "4\t○═╧═╧═●═╝\n",
      "\tTurn: BLACK, Game State (ONGOING|PASSED|END): ONGOING\n",
      "\tBlack Area: 3, White Area: 3\n",
      "\n",
      "Iteration:  0 Step:  6  Side:  Black action: \n",
      "\t0 1 2 3 4 \n",
      "0\t○═╤═○═●═●\n",
      "1\t○─┼─┼─┼─╢\n",
      "2\t╟─┼─┼─┼─╢\n",
      "3\t╟─┼─┼─┼─╢\n",
      "4\t○═╧═╧═●═╝\n",
      "\tTurn: WHITE, Game State (ONGOING|PASSED|END): ONGOING\n",
      "\tBlack Area: 4, White Area: 3\n",
      "\n",
      "Iteration:  0 Step:  7  Side:  White action: \n",
      "\t0 1 2 3 4 \n",
      "0\t○═╤═○═●═●\n",
      "1\t○─┼─┼─┼─╢\n",
      "2\t╟─┼─●─┼─╢\n",
      "3\t╟─┼─┼─┼─╢\n",
      "4\t○═╧═╧═●═╝\n",
      "\tTurn: BLACK, Game State (ONGOING|PASSED|END): ONGOING\n",
      "\tBlack Area: 4, White Area: 4\n",
      "\n",
      "Iteration:  0 Step:  8  Side:  Black action: \n",
      "\t0 1 2 3 4 \n",
      "0\t○═╤═○═●═●\n",
      "1\t○─┼─┼─○─╢\n",
      "2\t╟─┼─●─┼─╢\n",
      "3\t╟─┼─┼─┼─╢\n",
      "4\t○═╧═╧═●═╝\n",
      "\tTurn: WHITE, Game State (ONGOING|PASSED|END): ONGOING\n",
      "\tBlack Area: 5, White Area: 4\n",
      "\n",
      "Iteration:  0 Step:  9  Side:  White action: \n",
      "\t0 1 2 3 4 \n",
      "0\t○═╤═○═●═●\n",
      "1\t○─●─┼─○─╢\n",
      "2\t╟─┼─●─┼─╢\n",
      "3\t╟─┼─┼─┼─╢\n",
      "4\t○═╧═╧═●═╝\n",
      "\tTurn: BLACK, Game State (ONGOING|PASSED|END): ONGOING\n",
      "\tBlack Area: 5, White Area: 5\n",
      "\n",
      "Iteration:  0 Step:  10  Side:  Black action: \n",
      "\t0 1 2 3 4 \n",
      "0\t○═╤═○═╤═╗\n",
      "1\t○─●─┼─○─○\n",
      "2\t╟─┼─●─┼─╢\n",
      "3\t╟─┼─┼─┼─╢\n",
      "4\t○═╧═╧═●═╝\n",
      "\tTurn: WHITE, Game State (ONGOING|PASSED|END): ONGOING\n",
      "\tBlack Area: 8, White Area: 3\n",
      "\n",
      "Iteration:  0 Step:  11  Side:  White action: \n",
      "\t0 1 2 3 4 \n",
      "0\t○═●═○═╤═╗\n",
      "1\t○─●─┼─○─○\n",
      "2\t╟─┼─●─┼─╢\n",
      "3\t╟─┼─┼─┼─╢\n",
      "4\t○═╧═╧═●═╝\n",
      "\tTurn: BLACK, Game State (ONGOING|PASSED|END): ONGOING\n",
      "\tBlack Area: 8, White Area: 4\n",
      "\n",
      "Iteration:  0 Step:  12  Side:  Black action: \n",
      "\t0 1 2 3 4 \n",
      "0\t○═●═○═╤═╗\n",
      "1\t○─●─┼─○─○\n",
      "2\t╟─┼─●─┼─╢\n",
      "3\t╟─┼─┼─┼─╢\n",
      "4\t○═╧═╧═●═╝\n",
      "\tTurn: WHITE, Game State (ONGOING|PASSED|END): PASSED\n",
      "\tBlack Area: 8, White Area: 4\n",
      "\n",
      "Iteration:  0 Step:  13  Side:  White action: \n",
      "\t0 1 2 3 4 \n",
      "0\t○═●═○═●═╗\n",
      "1\t○─●─┼─○─○\n",
      "2\t╟─┼─●─┼─╢\n",
      "3\t╟─┼─┼─┼─╢\n",
      "4\t○═╧═╧═●═╝\n",
      "\tTurn: BLACK, Game State (ONGOING|PASSED|END): ONGOING\n",
      "\tBlack Area: 6, White Area: 5\n",
      "\n",
      "Iteration:  0 Step:  14  Side:  Black action: \n",
      "\t0 1 2 3 4 \n",
      "0\t○═●═○═╤═○\n",
      "1\t○─●─┼─○─○\n",
      "2\t╟─┼─●─┼─╢\n",
      "3\t╟─┼─┼─┼─╢\n",
      "4\t○═╧═╧═●═╝\n",
      "\tTurn: WHITE, Game State (ONGOING|PASSED|END): ONGOING\n",
      "\tBlack Area: 8, White Area: 4\n",
      "\n",
      "Iteration:  0 Step:  15  Side:  White action: \n",
      "\t0 1 2 3 4 \n",
      "0\t○═●═○═╤═○\n",
      "1\t○─●─┼─○─○\n",
      "2\t╟─┼─●─●─╢\n",
      "3\t╟─┼─┼─┼─╢\n",
      "4\t○═╧═╧═●═╝\n",
      "\tTurn: BLACK, Game State (ONGOING|PASSED|END): ONGOING\n",
      "\tBlack Area: 8, White Area: 5\n",
      "\n",
      "Iteration:  0 Step:  16  Side:  Black action: \n",
      "\t0 1 2 3 4 \n",
      "0\t○═●═○═╤═○\n",
      "1\t○─●─┼─○─○\n",
      "2\t○─┼─●─●─╢\n",
      "3\t╟─┼─┼─┼─╢\n",
      "4\t○═╧═╧═●═╝\n",
      "\tTurn: WHITE, Game State (ONGOING|PASSED|END): ONGOING\n",
      "\tBlack Area: 9, White Area: 5\n",
      "\n",
      "Iteration:  0 Step:  17  Side:  White action: \n",
      "\t0 1 2 3 4 \n",
      "0\t○═●═○═╤═○\n",
      "1\t○─●─┼─○─○\n",
      "2\t○─┼─●─●─●\n",
      "3\t╟─┼─┼─┼─╢\n",
      "4\t○═╧═╧═●═╝\n",
      "\tTurn: BLACK, Game State (ONGOING|PASSED|END): ONGOING\n",
      "\tBlack Area: 9, White Area: 6\n",
      "\n",
      "Iteration:  0 Step:  18  Side:  Black action: \n",
      "\t0 1 2 3 4 \n",
      "0\t○═●═○═○═○\n",
      "1\t○─●─┼─○─○\n",
      "2\t○─┼─●─●─●\n",
      "3\t╟─┼─┼─┼─╢\n",
      "4\t○═╧═╧═●═╝\n",
      "\tTurn: WHITE, Game State (ONGOING|PASSED|END): ONGOING\n",
      "\tBlack Area: 9, White Area: 6\n",
      "\n",
      "Iteration:  0 Step:  19  Side:  White action: \n",
      "\t0 1 2 3 4 \n",
      "0\t○═●═╤═╤═╗\n",
      "1\t○─●─●─┼─╢\n",
      "2\t○─┼─●─●─●\n",
      "3\t╟─┼─┼─┼─╢\n",
      "4\t○═╧═╧═●═╝\n",
      "\tTurn: BLACK, Game State (ONGOING|PASSED|END): ONGOING\n",
      "\tBlack Area: 4, White Area: 12\n",
      "\n",
      "Iteration:  0 Step:  20  Side:  Black action: \n",
      "\t0 1 2 3 4 \n",
      "0\t○═●═╤═╤═○\n",
      "1\t○─●─●─┼─╢\n",
      "2\t○─┼─●─●─●\n",
      "3\t╟─┼─┼─┼─╢\n",
      "4\t○═╧═╧═●═╝\n",
      "\tTurn: WHITE, Game State (ONGOING|PASSED|END): ONGOING\n",
      "\tBlack Area: 5, White Area: 7\n",
      "\n",
      "Iteration:  0 Step:  21  Side:  White action: \n",
      "\t0 1 2 3 4 \n",
      "0\t○═●═●═╤═○\n",
      "1\t○─●─●─┼─╢\n",
      "2\t○─┼─●─●─●\n",
      "3\t╟─┼─┼─┼─╢\n",
      "4\t○═╧═╧═●═╝\n",
      "\tTurn: BLACK, Game State (ONGOING|PASSED|END): ONGOING\n",
      "\tBlack Area: 5, White Area: 8\n",
      "\n",
      "Iteration:  0 Step:  22  Side:  Black action: \n",
      "\t0 1 2 3 4 \n",
      "0\t○═●═●═╤═○\n",
      "1\t○─●─●─┼─○\n",
      "2\t○─┼─●─●─●\n",
      "3\t╟─┼─┼─┼─╢\n",
      "4\t○═╧═╧═●═╝\n",
      "\tTurn: WHITE, Game State (ONGOING|PASSED|END): ONGOING\n",
      "\tBlack Area: 6, White Area: 8\n",
      "\n",
      "Iteration:  0 Step:  23  Side:  White action: \n",
      "\t0 1 2 3 4 \n",
      "0\t○═●═●═╤═○\n",
      "1\t○─●─●─┼─○\n",
      "2\t○─┼─●─●─●\n",
      "3\t╟─┼─┼─┼─╢\n",
      "4\t○═╧═●═●═╝\n",
      "\tTurn: BLACK, Game State (ONGOING|PASSED|END): ONGOING\n",
      "\tBlack Area: 6, White Area: 9\n",
      "\n",
      "Iteration:  0 Step:  24  Side:  Black action: \n",
      "\t0 1 2 3 4 \n",
      "0\t○═●═●═╤═○\n",
      "1\t○─●─●─┼─○\n",
      "2\t○─┼─●─●─●\n",
      "3\t╟─┼─┼─┼─○\n",
      "4\t○═╧═●═●═╝\n",
      "\tTurn: WHITE, Game State (ONGOING|PASSED|END): ONGOING\n",
      "\tBlack Area: 7, White Area: 9\n",
      "\n",
      "Iteration:  0 Step:  25  Side:  White action: \n",
      "\t0 1 2 3 4 \n",
      "0\t○═●═●═╤═○\n",
      "1\t○─●─●─┼─○\n",
      "2\t○─┼─●─●─●\n",
      "3\t╟─┼─┼─┼─○\n",
      "4\t○═╧═●═●═╝\n",
      "\tTurn: BLACK, Game State (ONGOING|PASSED|END): PASSED\n",
      "\tBlack Area: 7, White Area: 9\n",
      "\n",
      "Iteration:  0 Step:  26  Side:  Black action: \n",
      "\t0 1 2 3 4 \n",
      "0\t○═●═●═╤═○\n",
      "1\t○─●─●─┼─○\n",
      "2\t○─┼─●─●─●\n",
      "3\t╟─┼─┼─┼─○\n",
      "4\t○═╧═●═●═╝\n",
      "\tTurn: WHITE, Game State (ONGOING|PASSED|END): END\n",
      "\tBlack Area: 7, White Area: 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alphago_zero.max_moves = 100\n",
    "\n",
    "alphago_zero.train(1, render=True, eval=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
