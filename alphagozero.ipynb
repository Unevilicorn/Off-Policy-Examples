{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gym\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [black, white, turn, invalid, pass, game_over]\n",
    "class MCTSNode:\n",
    "    def __init__(self, state, parent=None, prior_action=-1, device='cpu'):\n",
    "        size = state.shape[1] * state.shape[2] + 1 # board size + pass\n",
    "        self.valid_actions = ~state[3]\n",
    "        self.visit_counts = torch.zeros(size).to(device)\n",
    "        self.total_action_values = torch.zeros(size).to(device)\n",
    "        self.mean_action_values = torch.zeros(size).to(device)\n",
    "        self.prior_probabilities = torch.zeros(size).to(device)\n",
    "        \n",
    "        self.selected_count = 0\n",
    "        self.children = torch.empty(size, dtype=torch.object)\n",
    "        \n",
    "        self.parent = parent\n",
    "        self.prior_action = prior_action\n",
    "        \n",
    "    def select(self, puct_multiplier=1.0):\n",
    "        const_part = puct_multiplier * torch.sqrt(self.selected_count)\n",
    "        us = const_part * self.prior_probabilities / (1 + self.visit_counts)\n",
    "        \n",
    "        sums = self.mean_action_values + us\n",
    "        masked_sums = sums * self.valid_actions\n",
    "        max_a = torch.argmax(masked_sums)\n",
    "        \n",
    "        self.selected_count += 1\n",
    "        self.visit_counts[max_a] += 1\n",
    "        \n",
    "        return max_a, self.children[max_a]\n",
    "    \n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, evaluator, board_size=19, device='cpu'):\n",
    "        self.root = MCTSNode(None)\n",
    "        self.PUCT_CONST = 1.0\n",
    "        self.tau = 1.0\n",
    "        \n",
    "        # dihedral transforms on the board\n",
    "        self.dihedrals_transforms = [\n",
    "            lambda x: x,\n",
    "            lambda x: torch.rot90(x, k=1, axes=(1, 2)),\n",
    "            lambda x: torch.rot90(x, k=2, axes=(1, 2)),\n",
    "            lambda x: torch.rot90(x, k=3, axes=(1, 2)),\n",
    "            lambda x: torch.flip(x, axis=1),\n",
    "            lambda x: torch.flip(x, axis=2),\n",
    "            lambda x: torch.flip(torch.rot90(x, k=1, axes=(1, 2)), axis=1),\n",
    "            lambda x: torch.flip(torch.rot90(x, k=1, axes=(1, 2)), axis=2),\n",
    "        ]\n",
    "        \n",
    "        self.evaluator = evaluator\n",
    "        self.board_size = board_size\n",
    "        self.device = device\n",
    "    \n",
    "    def train(self, env, state, deterministic=False):\n",
    "        turn = state[2][0][0]        \n",
    "        \n",
    "        state, parent, child, action, done, reward = self.search(env, state, self.root)\n",
    "        if done:\n",
    "            mul = 1 if state[2][0][0] != turn else -1\n",
    "            value = reward * mul\n",
    "        else:\n",
    "            child, value = self.expand(state, parent, child, action)\n",
    "        self.backup(child, value)\n",
    "                \n",
    "    def search(self, env, root):\n",
    "        node = root\n",
    "        cnode = root\n",
    "        state = None\n",
    "        while cnode and not done:\n",
    "            node = cnode\n",
    "            action, cnode = node.select(self.PUCT_CONST, device=self.device)\n",
    "            state, reward, done, _ = env.step(action)            \n",
    "        \n",
    "        return state, node, cnode, action, done, reward\n",
    "\n",
    "    def expand(self, state, parent, child, action):\n",
    "        dihedrals = [t(state) for t in self.dihedrals_transforms]\n",
    "        dihedrals = torch.tensor(dihedrals).to(self.device)\n",
    "        policy, value = self.evaluator(dihedrals)\n",
    "        # average policy and value over dihedral transforms\n",
    "        policy = policy.mean(dim=0)\n",
    "        value = value.mean()\n",
    "        \n",
    "        child = MCTSNode(state, parent=parent, action=action, device=self.device)\n",
    "        parent.children[action] = child\n",
    "        child.prior_probabilities = policy\n",
    "        \n",
    "        return child, value\n",
    "        \n",
    "    def backup(self, child, value):\n",
    "        child.visit_counts += 1\n",
    "        parent = child.parent\n",
    "        while parent:\n",
    "            parent.selected_count += 1\n",
    "            parent.visited_counts[child.prior_action] += 1\n",
    "            \n",
    "            parent.total_action_values[child.prior_action] += value\n",
    "            parent.mean_action_values[child.prior_action] = parent.total_action_values[child.prior_action] / parent.visit_counts[child.prior_action]\n",
    "            \n",
    "            \n",
    "    def play(self, state, node, deterministic=False):\n",
    "        pis = (node.visit_counts ** (1 / self.tau)) / (node.selected_count ** (1 / self.tau))\n",
    "        if deterministic:\n",
    "            selected_action = torch.argmax(pis)\n",
    "        else:\n",
    "            selected_action = torch.random.choice(pis)\n",
    "        child = node.children[selected_action]\n",
    "        child.parent = None\n",
    "        return selected_action\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, input_dims, n_filters):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(input_dims, n_filters, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(n_filters),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(n_filters, n_filters, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(n_filters),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.shortcut = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(input_dims, n_filters, kernel_size=1, stride=1),\n",
    "            torch.nn.BatchNorm2d(n_filters),\n",
    "        )\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.layers(x) + self.shortcut(x))\n",
    "\n",
    "\n",
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, board_size, input_dims):\n",
    "        super().__init__()\n",
    "        n_filters = 64\n",
    "        \n",
    "        self.main_path = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(input_dims[0], 256, kernel_size=3, stride=1),\n",
    "            torch.nn.BatchNorm2d(n_filters),\n",
    "            torch.nn.ReLU(),\n",
    "            ResidualBlock(n_filters, n_filters),\n",
    "            ResidualBlock(n_filters, n_filters),\n",
    "            ResidualBlock(n_filters, n_filters),\n",
    "            ResidualBlock(n_filters, n_filters),\n",
    "            ResidualBlock(n_filters, n_filters),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.policy = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(n_filters, 2, kernel_size=1, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear((board_size-2)**2 * 2, board_size ** 2 + 1),\n",
    "            torch.nn.Softmax(dim=1),\n",
    "        )\n",
    "        \n",
    "        self.value = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(n_filters, 1, kernel_size=1, stride=1),\n",
    "            torch.nn.BatchNorm2d(1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear((board_size-2)**2, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 1),\n",
    "            torch.nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = self.main_path(state)\n",
    "        policy = self.policy(x)\n",
    "        value = self.value(x)\n",
    "        return policy, value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', 'state policy outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaGoZero:\n",
    "    def __init__(self, env, board_size=19, device='cpu'):\n",
    "        self.env = env\n",
    "        self.device = device\n",
    "        \n",
    "        self.sub_iterations = 100\n",
    "        self.batch_size = 16\n",
    "        self.min_buffer_size = 16\n",
    "                \n",
    "        input_dims = env.observation_space.shape\n",
    "        \n",
    "        self.dnn = DNN(board_size, input_dims).to(device)\n",
    "        self.mcts = MCTS(self.dnn, board_size, device)\n",
    "        self.trojectory_buffer = deque(maxlen=10000)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.dnn.parameters(), lr=0.0001)\n",
    "        \n",
    "    def train(self, iterations):\n",
    "        for i in range(iterations):\n",
    "\n",
    "            state = self.env.reset()\n",
    "            state = torch.tensor(state).to(self.device)\n",
    "            done = False\n",
    "        \n",
    "            trajectory_white = []\n",
    "            trajectory_black = []\n",
    "            side = 1\n",
    "            while not done:\n",
    "                for _ in range(self.sub_iterations):\n",
    "                    env2 = copy.deepcopy(self.env)\n",
    "                    self.mcts.train(env2, state)\n",
    "                    env2.close()\n",
    "                \n",
    "                action = self.mtc.play(state, deterministic=False)  \n",
    "                nstate, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                if side == 1:\n",
    "                    trajectory_white.append(Experience(state, action, reward))\n",
    "                else:\n",
    "                    trajectory_black.append(Experience(state, action, reward))\n",
    "                \n",
    "                state = torch.tensor(nstate).to(self.device)\n",
    "                \n",
    "                side *= -1\n",
    "            \n",
    "            outcome = reward\n",
    "            \n",
    "            for exp in trajectory_white:\n",
    "                self.trojectory_buffer.append(Experience(exp.state, exp.action, -outcome))\n",
    "            for exp in trajectory_black:\n",
    "                self.trojectory_buffer.append(Experience(exp.state, exp.action, outcome))\n",
    "\n",
    "            self.train_dnn()\n",
    "            \n",
    "    def train_dnn(self):\n",
    "        if len(self.trojectory_buffer) < self.min_buffer_size:\n",
    "            return\n",
    "        \n",
    "        batches = random.sample(self.trojectory_buffer, self.batch_size)\n",
    "        states, policies, outcomes = zip(*batches)\n",
    "        states = torch.stack(states).to(self.device).detach()\n",
    "        policies = torch.tensor(policies).to(self.device).detach()\n",
    "        outcomes = torch.tensor(outcomes).to(self.device).detach()\n",
    "        \n",
    "        winners = np.array(outcomes)[:, -1]\n",
    "\n",
    "        # Forward pass\n",
    "        predicted_policies, predicted_values = self.dnn(states)\n",
    "        \n",
    "        # Define loss function (e.g., MSE for value and cross-entropy for policy)\n",
    "        value_loss = torch.nn.functional.mse_loss(predicted_values.squeeze(-1), outcomes)\n",
    "        policy_loss = torch.nn.functional.cross_entropy(predicted_policies, policies)\n",
    "        total_loss = value_loss + policy_loss\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 6, 7, 7])\n",
      "torch.Size([3, 50]) torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "SIZE = 7\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "env = gym.make('gym_go:go-v0', size=7, komi=0, reward_method='real')\n",
    "\n",
    "alphago_zero = AlphaGoZero(env, board_size=7, device=device)\n",
    "alphago_zero.train(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
